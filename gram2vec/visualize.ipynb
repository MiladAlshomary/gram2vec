{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "This notebook is meant for visualizing stuff and testing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from nltk import Tree\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "from more_itertools import chunked\n",
    "from dataclasses import dataclass\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from featurizers import GrammarVectorizer, make_document\n",
    "from typing import List, Dict, Tuple, Set\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAN 2022 Summary Stats & Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Author:\n",
    "    \"\"\"\n",
    "    Stores author information in an easy to work with format\n",
    "    \n",
    "    :param author_id: unique author id\n",
    "    :param fixed_texts: list of author documents with regex fixes\n",
    "    :param raw_texts: list of author documents without regex fixes\n",
    "    :param discourse_types: list of discourse types\n",
    "    \n",
    "    Note: fixed_docs, raw_docs, and discourse_types are all 1 - 1 corresponding\n",
    "    \"\"\"\n",
    "    author_id:str\n",
    "    fixed_texts:list[str]\n",
    "    raw_texts:list[str]\n",
    "    discourse_types:list[str]\n",
    "    \n",
    "    def get_token_counts(self) -> list[int]:\n",
    "        return [len(word_tokenize(author_doc)) for author_doc in self.fixed_texts]\n",
    "    \n",
    "    def get_total_docs(self) -> int:\n",
    "        return len(self.fixed_texts)\n",
    "    \n",
    "    def count_dicourse_type(self, dtype:str) -> int:\n",
    "        return Counter(self.discourse_types)[dtype]\n",
    "        \n",
    "def load_json(path:str) -> dict[str, list[dict]]:\n",
    "    with open(path, \"r\") as fin:\n",
    "        data = json.load(fin)\n",
    "        return data\n",
    "\n",
    "def extract_from_dict(author_entry:dict, to_extract:str) -> list[str]:\n",
    "    return [entry[to_extract] for entry in author_entry]\n",
    "    \n",
    "def create_author_list(preprocessed_data:dict[str, list[dict]]) -> list[Author]:\n",
    "    \"\"\"\n",
    "    Converts the preprocessed data into a list of Author objects\n",
    "    \"\"\"\n",
    "    authors = []\n",
    "    for author_id in preprocessed_data.keys():\n",
    "        author_entry = preprocessed_data[author_id]\n",
    "        fixed_texts = extract_from_dict(author_entry,\"fixed_text\")\n",
    "        raw_texts = extract_from_dict(author_entry,\"raw_text\")\n",
    "        discourse_types = extract_from_dict(author_entry,\"discourse_type\")\n",
    "            \n",
    "        authors.append(Author(author_id, fixed_texts, raw_texts, discourse_types))\n",
    "        \n",
    "    return authors\n",
    "\n",
    "def get_doc_token_stats(authors:list[Author]) -> tuple[float, float]:\n",
    "    \"\"\"Gets the mean and std of tokens per document\"\"\"\n",
    "    all_doc_token_counts = []\n",
    "    for author in authors:\n",
    "        all_doc_token_counts.extend(author.get_token_counts())\n",
    "    return np.mean(all_doc_token_counts), np.std(all_doc_token_counts)\n",
    "    \n",
    "def make_author_df(authors:list[Author]) -> pd.DataFrame:\n",
    "    \n",
    "    author_maps = defaultdict(list)\n",
    "    for author in authors:\n",
    "        author_maps[\"author_id\"].append(author.author_id)\n",
    "        author_maps[\"total_token_count\"].append(sum(author.get_token_counts()))\n",
    "        author_maps[\"Total docs\"].append(author.get_total_docs())\n",
    "        author_maps[\"Emails\"].append(author.count_dicourse_type(\"email\"))\n",
    "        author_maps[\"Memos\"].append(author.count_dicourse_type(\"memo\"))\n",
    "        author_maps[\"Txt msgs\"].append(author.count_dicourse_type(\"text_message\"))\n",
    "        author_maps[\"Essays\"].append(author.count_dicourse_type(\"essay\"))\n",
    "        \n",
    "    return pd.DataFrame(author_maps)\n",
    "\n",
    "data = load_json(\"data/pan22/preprocessed/author_doc_mappings.json\")\n",
    "all_authors = create_author_list(data)\n",
    "df = make_author_df(all_authors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2v = GrammarVectorizer()\n",
    "\n",
    "def get_all_documents(data_path:str, text_type=\"fixed_text\") -> list[str]:\n",
    "    \"\"\"Aggregates all documents into one list\"\"\"\n",
    "    all_documents = []\n",
    "    for author_entries in load_json(data_path).values():\n",
    "        for entry in author_entries:\n",
    "            all_documents.append(make_document(entry[text_type], g2v.nlp))\n",
    "            \n",
    "    return all_documents\n",
    "\n",
    "all_documents = get_all_documents(\"eval/pan22_splits/knn/train.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic construction featues?\n",
    "\n",
    "- Look for spacy tree pattern matcher online\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 are_AUX_ROOT                                   \n",
      "        ______________|_______________                           \n",
      "       |                       Apples_NOUN_nsub                 \n",
      "       |                              j                         \n",
      "       |                              |                          \n",
      "       |                       likes_VERB_relcl                 \n",
      "       |               _______________|________________          \n",
      "tasty_ADJ_acomp that_PRON_dobj                  John_PROPN_nsubj\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Apples that fall from trees taste good\"\n",
    "sentence = \"Apples that John likes are tasty\"\n",
    "nlp = g2v.nlp\n",
    "doc = nlp(sentence)\n",
    "\n",
    "def _to_nltk_tree(node):\n",
    "    \n",
    "    _tok_format = lambda tok: \"_\".join([tok.orth_, tok.pos_,tok.dep_])\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(_tok_format(node), [_to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return _tok_format(node)\n",
    "\n",
    "def get_nltk_tree(doc):\n",
    "    #https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy\n",
    "    return [_to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "\n",
    "def get_root(doc) -> spacy.tokens.token.Token:\n",
    "    return [token for token in doc if token.head == token][0]\n",
    "\n",
    "def get_subject(root) -> spacy.tokens.token.Token:\n",
    "    return list(root.lefts)[0]\n",
    "\n",
    "\n",
    "# root = get_root(doc)\n",
    "# subject = get_subject(root)\n",
    "\n",
    "# for descendant in subject.subtree:\n",
    "\n",
    "#     print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
    "#             descendant.n_rights,\n",
    "#             [ancestor.text for ancestor in descendant.ancestors])\n",
    "\n",
    "get_nltk_tree(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "dobj_relcl = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary richness vector?\n",
    "\n",
    "- hapaxes\n",
    "- \\# of mispelled words?\n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.scripts.pan_create_bins import get_train_authors_sorted_by_docfreq\n",
    "\n",
    "def bin_authors(iterable) -> tuple[list[str], ...]:\n",
    "    return tuple(chunked(iterable, 7)) \n",
    "\n",
    "def make_doc_avg_labels(sorted_dict):\n",
    "       \n",
    "       labels = []\n",
    "       for bin in bin_authors(list(sorted_dict.values())):\n",
    "              labels.append(round(np.mean(bin), 2))\n",
    "       return labels\n",
    "\n",
    "train_path = \"eval/pan22_splits/knn/train.json\"\n",
    "train_authors_sorted = get_train_authors_sorted_by_docfreq(train_path)\n",
    "labels = make_doc_avg_labels(train_authors_sorted)\n",
    "\n",
    "# k = 6\n",
    "\n",
    "r_at_1 = np.array([0.02857142857,0.2285714286,0.1428571429,0.1428571429,0.2285714286,0.1142857143,0.2285714286,0.5142857143])\n",
    "\n",
    "r_at_8 = np.array([0.2571428571,0.5142857143,0.5142857143,0.5142857143,0.7142857143,0.6857142857,0.7142857143,0.7428571429,])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "       \"Bin labels\": labels,\n",
    "       \"R@1\": r_at_1,\n",
    "       \"R@8\": r_at_8\n",
    "})\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "sns.lineplot(data=df, x=\"Bin labels\", y=\"R@1\",color=\"blue\",marker=\"o\", label=\"R@1\")\n",
    "sns.lineplot(data=df, x=\"Bin labels\", y=\"R@8\",color=\"green\",marker=\"o\", label=\"R@8\")\n",
    "plt.xlabel(\"Avg document count\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Binned author scores\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "df[\"Total docs\"].hist(bins=7)\n",
    "plt.title(\"Document counts per author\")\n",
    "plt.xlabel(\"# of documents\")\n",
    "plt.ylabel(\"# of authors\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains deprecated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATS_ACCS = [0.0, 0.05714285714285714, 0.11428571428571428, 0.17142857142857143, 0.2, 0.2, 0.2857142857142857, 0.5428571428571428]\n",
    "HALF_FEATS_ACCS = [0.0, 0.02857142857142857, 0.08571428571428572, 0.11428571428571428, 0.22857142857142856, 0.22857142857142856, 0.22857142857142856, 0.4]\n",
    "\n",
    "old_df = pd.DataFrame(\n",
    "    {\"Full features\": ALL_FEATS_ACCS,\n",
    "     \"Half features\": HALF_FEATS_ACCS,\n",
    "     \"Bin labels\":labels}\n",
    ")\n",
    "\n",
    "\n",
    "sns.lineplot(data=old_df, x=\"Bin labels\", y=\"Full features\",color=\"blue\", label=\"Full features\")\n",
    "sns.lineplot(data=old_df, x=\"Bin labels\", y=\"Half features\", color=\"red\", label=\"Half features\")\n",
    "plt.xlabel(\"Avg document count\")\n",
    "plt.ylabel(\"R@1 score\")\n",
    "plt.title(\"R@1 Development bin scores\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAN 2022 Discourse related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_discourse_types(path:str) -> tuple[list,list,list]:\n",
    "    \"\"\"Loads the preprocessed data and sorts it by discourse type\"\"\"\n",
    "    preprocessed = load_json(path)\n",
    "    author_ids = preprocessed.keys()\n",
    "    emails = []\n",
    "    memos = []\n",
    "    txt_msgs = []\n",
    "    essays = []\n",
    "    for author_id in author_ids:\n",
    "        for author_entry in preprocessed[author_id]:\n",
    "            dtype = author_entry[\"discourse_type\"]\n",
    "            fixed = author_entry[\"fixed_text\"].split()\n",
    "            \n",
    "            if  dtype == \"email\":\n",
    "                emails.append(fixed)\n",
    "                \n",
    "            if  dtype == \"memo\":\n",
    "                memos.append(fixed)\n",
    "                \n",
    "            if  dtype == \"text_message\":\n",
    "                txt_msgs.append(fixed)\n",
    "                \n",
    "            if  dtype == \"essay\":\n",
    "                essays.append(fixed)\n",
    "    return emails, memos, txt_msgs, essays\n",
    "  \n",
    "def get_avg_tokens(dtype:list[list[str]]) -> int:\n",
    "    \n",
    "    token_counts = []\n",
    "    for tokens in dtype:\n",
    "        token_counts.append(len(tokens))\n",
    "    return np.mean(token_counts)\n",
    "              \n",
    "            \n",
    "emails, memos, txt_msgs, essays = load_all_discourse_types(\"data/pan22/preprocessed/preprocessed_data.json\")\n",
    "\n",
    "\n",
    "print(get_avg_tokens(emails))\n",
    "print(get_avg_tokens(memos))\n",
    "print(get_avg_tokens(txt_msgs))\n",
    "print(get_avg_tokens(essays))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "dtype_df = df[[\"Emails\", \"Txt msgs\", \"Essays\", \"Memos\"]].sum()\n",
    "dtype_df.plot.bar(color=[\"teal\", \"lightpink\", \"orange\", \"brown\"])\n",
    "plt.title(\"Discourse type counts\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blogs testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_preprocessed = pd.read_csv(\"data/blogs/preprocessed/blogs_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tkn_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526798</th>\n",
       "      <td>76211</td>\n",
       "      <td>M.ardi G.ras Update + Summer Don't...</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526703</th>\n",
       "      <td>76211</td>\n",
       "      <td>Happy Anniversary to Me   Today ma...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642373</th>\n",
       "      <td>122217</td>\n",
       "      <td>Her name is Sugar. The  urlLink 4yo Rot...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642426</th>\n",
       "      <td>122217</td>\n",
       "      <td>Wednesday night was an orgy of caloric ...</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37184</th>\n",
       "      <td>152151</td>\n",
       "      <td>yay heading up to berkeley this aft...</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583186</th>\n",
       "      <td>3333298</td>\n",
       "      <td>This post also features news ...</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96357</th>\n",
       "      <td>3344990</td>\n",
       "      <td>You have got to love training.  ...</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96331</th>\n",
       "      <td>3344990</td>\n",
       "      <td>First there was  Will &amp; Grace , n...</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170032</th>\n",
       "      <td>3456542</td>\n",
       "      <td>So, Whoopi Goldberg has been let go by ...</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169996</th>\n",
       "      <td>3456542</td>\n",
       "      <td>That wacky  urlLink Michael Moore  is s...</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_id                                               text  \\\n",
       "526798      76211              M.ardi G.ras Update + Summer Don't...   \n",
       "526703      76211              Happy Anniversary to Me   Today ma...   \n",
       "642373     122217         Her name is Sugar. The  urlLink 4yo Rot...   \n",
       "642426     122217         Wednesday night was an orgy of caloric ...   \n",
       "37184      152151             yay heading up to berkeley this aft...   \n",
       "...           ...                                                ...   \n",
       "583186    3333298                   This post also features news ...   \n",
       "96357     3344990                You have got to love training.  ...   \n",
       "96331     3344990               First there was  Will & Grace , n...   \n",
       "170032    3456542         So, Whoopi Goldberg has been let go by ...   \n",
       "169996    3456542         That wacky  urlLink Michael Moore  is s...   \n",
       "\n",
       "        tkn_count  \n",
       "526798        397  \n",
       "526703        110  \n",
       "642373        394  \n",
       "642426        238  \n",
       "37184         225  \n",
       "...           ...  \n",
       "583186        458  \n",
       "96357         609  \n",
       "96331         957  \n",
       "170032        517  \n",
       "169996        389  \n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def select_from_threshold(series:pd.Series, threshold:int) -> Set[int]:\n",
    "    \"\"\"Takes a pandas Series and selects indices that meet a given threshold\"\"\"\n",
    "    return set(series[series > threshold].index.to_list())\n",
    "\n",
    "def extract_blog_authors(blogs_df:pd.DataFrame, avg_tok_threshold:int, doc_threshold:int) -> pd.DataFrame:\n",
    "    \"\"\"Selects authors from the blogs df that meet an avg token count threshold and doc frequency threshold\"\"\"\n",
    "    \n",
    "    author_avg_tkns = blogs_df.groupby(\"id\")[\"tkn_count\"].mean()\n",
    "    author_doc_cnts= blogs_df.groupby(\"id\")[\"id\"].count()\n",
    "\n",
    "    selected_avg_tkn_authors = select_from_threshold(author_avg_tkns, avg_tok_threshold)\n",
    "    selected_doc_cnt_authors = select_from_threshold(author_doc_cnts, doc_threshold)\n",
    "    selected_ids = selected_avg_tkn_authors.intersection(selected_doc_cnt_authors)\n",
    "    \n",
    "    return blogs_df.loc[blogs_df[\"id\"].isin(selected_ids)]\n",
    "\n",
    "    \n",
    "df_sample = extract_blog_authors(blogs_df = blogs_preprocessed, \n",
    "                                 avg_tok_threshold = 350, \n",
    "                                 doc_threshold = 200)\n",
    "df_sample = df_sample.rename(columns={\"id\":\"author_id\"})\n",
    "\n",
    "\n",
    "\n",
    "#df_sample.groupby('author_id', group_keys=False).apply(lambda x: x.sample(min(len(x), 2)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8426aeb6a0394c95a6dca738b4382d3e4f73a60ab3fca776cba99777e8eb1027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
