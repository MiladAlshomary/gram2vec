{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "This notebook is meant for visualizing stuff and testing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from more_itertools import chunked\n",
    "from dataclasses import dataclass\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from gram2vec.featurizers import GrammarVectorizer, make_document\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAN 2022 Summary Stats & Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Author:\n",
    "    \"\"\"\n",
    "    Stores author information in an easy to work with format\n",
    "    \n",
    "    :param author_id: unique author id\n",
    "    :param fixed_texts: list of author documents with regex fixes\n",
    "    :param raw_texts: list of author documents without regex fixes\n",
    "    :param discourse_types: list of discourse types\n",
    "    \n",
    "    Note: fixed_docs, raw_docs, and discourse_types are all 1 - 1 corresponding\n",
    "    \"\"\"\n",
    "    author_id:str\n",
    "    fixed_texts:list[str]\n",
    "    raw_texts:list[str]\n",
    "    discourse_types:list[str]\n",
    "    \n",
    "    def get_token_counts(self) -> list[int]:\n",
    "        return [len(word_tokenize(author_doc)) for author_doc in self.fixed_texts]\n",
    "    \n",
    "    def get_total_docs(self) -> int:\n",
    "        return len(self.fixed_texts)\n",
    "    \n",
    "    def count_dicourse_type(self, dtype:str) -> int:\n",
    "        return Counter(self.discourse_types)[dtype]\n",
    "        \n",
    "def load_json(path:str) -> dict[str, list[dict]]:\n",
    "    with open(path, \"r\") as fin:\n",
    "        data = json.load(fin)\n",
    "        return data\n",
    "\n",
    "def extract_from_dict(author_entry:dict, to_extract:str) -> list[str]:\n",
    "    return [entry[to_extract] for entry in author_entry]\n",
    "    \n",
    "def create_author_list(preprocessed_data:dict[str, list[dict]]) -> list[Author]:\n",
    "    \"\"\"\n",
    "    Converts the preprocessed data into a list of Author objects\n",
    "    \"\"\"\n",
    "    authors = []\n",
    "    for author_id in preprocessed_data.keys():\n",
    "        author_entry = preprocessed_data[author_id]\n",
    "        fixed_texts = extract_from_dict(author_entry,\"fixed_text\")\n",
    "        raw_texts = extract_from_dict(author_entry,\"raw_text\")\n",
    "        discourse_types = extract_from_dict(author_entry,\"discourse_type\")\n",
    "            \n",
    "        authors.append(Author(author_id, fixed_texts, raw_texts, discourse_types))\n",
    "        \n",
    "    return authors\n",
    "\n",
    "def get_doc_token_stats(authors:list[Author]) -> tuple[float, float]:\n",
    "    \"\"\"Gets the mean and std of tokens per document\"\"\"\n",
    "    all_doc_token_counts = []\n",
    "    for author in authors:\n",
    "        all_doc_token_counts.extend(author.get_token_counts())\n",
    "    return np.mean(all_doc_token_counts), np.std(all_doc_token_counts)\n",
    "    \n",
    "def make_author_df(authors:list[Author]) -> pd.DataFrame:\n",
    "    \n",
    "    author_maps = defaultdict(list)\n",
    "    for author in authors:\n",
    "        author_maps[\"author_id\"].append(author.author_id)\n",
    "        author_maps[\"total_token_count\"].append(sum(author.get_token_counts()))\n",
    "        author_maps[\"Total docs\"].append(author.get_total_docs())\n",
    "        author_maps[\"Emails\"].append(author.count_dicourse_type(\"email\"))\n",
    "        author_maps[\"Memos\"].append(author.count_dicourse_type(\"memo\"))\n",
    "        author_maps[\"Txt msgs\"].append(author.count_dicourse_type(\"text_message\"))\n",
    "        author_maps[\"Essays\"].append(author.count_dicourse_type(\"essay\"))\n",
    "        \n",
    "    return pd.DataFrame(author_maps)\n",
    "\n",
    "data = load_json(\"data/pan22/preprocessed/author_doc_mappings.json\")\n",
    "all_authors = create_author_list(data)\n",
    "df = make_author_df(all_authors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.48287113e-02, 7.99347471e-02, 7.17781403e-02, 6.19902121e-02,\n",
       "       2.61011419e-02, 6.68841762e-02, 8.15660685e-03, 1.66394780e-01,\n",
       "       4.89396411e-03, 2.44698206e-02, 1.33768352e-01, 6.52528548e-03,\n",
       "       8.80913540e-02, 3.09951060e-02, 0.00000000e+00, 1.37030995e-01,\n",
       "       3.26264274e-03, 4.89396411e-03, 3.87596899e-02, 3.25581395e-02,\n",
       "       3.87596899e-02, 3.87596899e-02, 2.17054264e-02, 4.80620155e-02,\n",
       "       2.48062016e-02, 2.48062016e-02, 3.25581395e-02, 1.70542636e-02,\n",
       "       2.48062016e-02, 2.32558140e-02, 2.94573643e-02, 2.01550388e-02,\n",
       "       1.39534884e-02, 2.32558140e-02, 1.08527132e-02, 1.24031008e-02,\n",
       "       1.70542636e-02, 4.65116279e-03, 2.01550388e-02, 1.55038760e-02,\n",
       "       6.20155039e-03, 1.08527132e-02, 4.65116279e-03, 6.20155039e-03,\n",
       "       1.24031008e-02, 1.24031008e-02, 9.30232558e-03, 1.24031008e-02,\n",
       "       1.70542636e-02, 1.39534884e-02, 3.10077519e-03, 1.08527132e-02,\n",
       "       7.75193798e-03, 0.00000000e+00, 1.08527132e-02, 7.75193798e-03,\n",
       "       0.00000000e+00, 9.30232558e-03, 1.08527132e-02, 4.65116279e-03,\n",
       "       3.10077519e-03, 7.75193798e-03, 6.20155039e-03, 1.08527132e-02,\n",
       "       7.75193798e-03, 1.39534884e-02, 1.39534884e-02, 7.75193798e-03,\n",
       "       1.22950820e-02, 1.22950820e-02, 1.63934426e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.02459016e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.22950820e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.19672131e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.04918033e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.09836066e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.09836066e-03,\n",
       "       1.22950820e-02, 8.19672131e-03, 8.19672131e-03, 1.63934426e-02,\n",
       "       1.22950820e-02, 8.19672131e-03, 0.00000000e+00, 4.09836066e-03,\n",
       "       4.09836066e-03, 4.50819672e-02, 8.19672131e-03, 8.19672131e-03,\n",
       "       0.00000000e+00, 8.19672131e-03, 1.22950820e-02, 8.19672131e-03,\n",
       "       1.63934426e-02, 0.00000000e+00, 4.09836066e-03, 0.00000000e+00,\n",
       "       4.09836066e-03, 4.09836066e-03, 4.50819672e-02, 0.00000000e+00,\n",
       "       8.60655738e-02, 3.68852459e-02, 2.04918033e-02, 1.22950820e-02,\n",
       "       4.09836066e-03, 4.09836066e-03, 2.45901639e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.27868852e-02, 8.19672131e-03, 4.09836066e-03,\n",
       "       4.50819672e-02, 1.22950820e-02, 4.09836066e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.09836066e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.09836066e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.50819672e-02, 8.19672131e-03, 4.09836066e-03, 0.00000000e+00,\n",
       "       2.45901639e-02, 4.09836066e-03, 2.04918033e-02, 8.19672131e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.09836066e-03, 0.00000000e+00,\n",
       "       4.09836066e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.09836066e-03, 4.09836066e-03, 0.00000000e+00, 1.22950820e-02,\n",
       "       4.09836066e-03, 0.00000000e+00, 0.00000000e+00, 8.19672131e-03,\n",
       "       0.00000000e+00, 4.09836066e-03, 0.00000000e+00, 4.09836066e-03,\n",
       "       4.09836066e-03, 0.00000000e+00, 4.09836066e-03, 0.00000000e+00,\n",
       "       1.22950820e-02, 4.09836066e-03, 0.00000000e+00, 8.19672131e-03,\n",
       "       1.22950820e-02, 0.00000000e+00, 4.09836066e-03, 2.04918033e-02,\n",
       "       1.22950820e-02, 4.09836066e-03, 1.22950820e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.09836066e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.26436782e-01, 2.75862069e-01, 2.29885057e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.29885057e-02,\n",
       "       1.72413793e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.14942529e-02, 9.19540230e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.29885057e-02, 0.00000000e+00, 0.00000000e+00, 2.52873563e-01,\n",
       "       7.49891634e-02, 1.04031209e-02, 2.25400954e-02, 3.59774599e-02,\n",
       "       1.26137841e-01, 2.64412657e-02, 2.12397052e-02, 4.33463372e-02,\n",
       "       5.37494582e-02, 3.46770698e-03, 1.12700477e-02, 5.24490681e-02,\n",
       "       3.29432163e-02, 5.07152146e-02, 8.71261378e-02, 2.34070221e-02,\n",
       "       4.33463372e-04, 6.02514088e-02, 5.07152146e-02, 8.10576506e-02,\n",
       "       4.33463372e-02, 9.96965756e-03, 2.25400954e-02, 1.73385349e-03,\n",
       "       3.38101430e-02, 0.00000000e+00, 8.66926745e-04, 8.66926745e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.33463372e-04, 8.66926745e-04,\n",
       "       4.33463372e-04, 2.16731686e-03, 9.53619419e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.33463372e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.30039012e-03, 2.16731686e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.66926745e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.89396411e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.68458781e-01, 3.31541219e-01, 4.19534050e+00,\n",
       "       2.33629148e+00, 1.91562500e+01, 1.40559360e+01, 3.29749104e-01,\n",
       "       5.22022838e-02, 4.89396411e-03, 3.26264274e-02, 2.28384992e-02,\n",
       "       8.31973899e-02, 0.00000000e+00, 3.42577488e-02, 0.00000000e+00,\n",
       "       1.63132137e-02, 4.89396411e-02, 1.63132137e-03, 0.00000000e+00,\n",
       "       2.61011419e-02, 3.09951060e-02, 2.44698206e-02, 2.12071778e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.52528548e-03,\n",
       "       6.52528548e-02, 5.70962480e-02, 0.00000000e+00, 8.15660685e-03,\n",
       "       2.12071778e-02, 0.00000000e+00, 8.15660685e-03, 3.26264274e-03,\n",
       "       1.46818923e-02, 1.14192496e-01, 1.63132137e-03, 6.52528548e-03,\n",
       "       1.63132137e-03, 0.00000000e+00, 9.78792822e-03, 6.19902121e-02,\n",
       "       1.46818923e-02, 0.00000000e+00, 0.00000000e+00, 7.34094617e-02,\n",
       "       6.52528548e-03, 8.97226754e-02, 1.63132137e-03, 1.63132137e-02,\n",
       "       1.79445351e-02, 4.45682451e-02, 2.50696379e-02, 3.06406685e-02,\n",
       "       3.06406685e-02, 2.22841226e-02, 1.67130919e-02, 1.39275766e-02,\n",
       "       1.94986072e-02, 8.35654596e-03, 1.11420613e-02, 2.22841226e-02,\n",
       "       2.78551532e-03, 1.11420613e-02, 5.57103064e-03, 2.78551532e-03,\n",
       "       1.94986072e-02, 1.11420613e-02, 1.39275766e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.11420613e-02, 0.00000000e+00, 2.78551532e-03,\n",
       "       8.35654596e-03, 5.57103064e-03, 5.57103064e-03, 5.57103064e-03,\n",
       "       8.35654596e-03, 2.78551532e-03, 5.57103064e-03, 2.78551532e-03,\n",
       "       5.57103064e-03, 2.78551532e-03, 5.57103064e-03, 5.57103064e-03,\n",
       "       2.78551532e-03, 0.00000000e+00, 5.57103064e-03, 2.78551532e-03,\n",
       "       0.00000000e+00, 2.78551532e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.78551532e-03, 5.57103064e-03, 1.39275766e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.35654596e-03, 2.78551532e-03])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2v = GrammarVectorizer()\n",
    "\n",
    "def get_all_documents(data_path:str, text_type=\"fixed_text\") -> list[str]:\n",
    "    \"\"\"Aggregates all documents into one list\"\"\"\n",
    "    all_documents = []\n",
    "    for author_entries in load_json(data_path).values():\n",
    "        for entry in author_entries:\n",
    "            all_documents.append(make_document(entry[text_type], g2v.nlp))\n",
    "            \n",
    "    return all_documents\n",
    "\n",
    "all_documents = get_all_documents(\"eval/pan22_splits/knn/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Ind': 7327})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_itertools import flatten\n",
    "\n",
    "def get_morph_tag_distr(documents:list, morph_tag:str) -> dict:\n",
    "    \n",
    "    morph_tag_map = defaultdict(int)\n",
    "    for document in documents:\n",
    "        doc_morphs = list(flatten([token.morph.get(morph_tag) for token in document.spacy_doc]))\n",
    "        for morph in doc_morphs:\n",
    "            morph_tag_map[morph] += 1\n",
    "    return morph_tag_map\n",
    "\n",
    "get_morph_tag_distr(all_documents, \"Mood\")\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.scripts.pan_create_bins import get_train_authors_sorted_by_docfreq\n",
    "\n",
    "def bin_authors(iterable) -> tuple[list[str], ...]:\n",
    "    return tuple(chunked(iterable, 7)) \n",
    "\n",
    "def make_doc_avg_labels(sorted_dict):\n",
    "       \n",
    "       labels = []\n",
    "       for bin in bin_authors(list(sorted_dict.values())):\n",
    "              labels.append(round(np.mean(bin), 2))\n",
    "       return labels\n",
    "\n",
    "train_path = \"eval/pan22_splits/knn/train.json\"\n",
    "train_authors_sorted = get_train_authors_sorted_by_docfreq(train_path)\n",
    "labels = make_doc_avg_labels(train_authors_sorted)\n",
    "\n",
    "# k = 6\n",
    "\n",
    "r_at_1 = np.array([0.02857142857,0.2285714286,0.1428571429,0.1428571429,0.2285714286,0.1142857143,0.2285714286,0.5142857143])\n",
    "\n",
    "r_at_8 = np.array([0.2571428571,0.5142857143,0.5142857143,0.5142857143,0.7142857143,0.6857142857,0.7142857143,0.7428571429,])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "       \"Bin labels\": labels,\n",
    "       \"R@1\": r_at_1,\n",
    "       \"R@8\": r_at_8\n",
    "})\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "sns.lineplot(data=df, x=\"Bin labels\", y=\"R@1\",color=\"blue\",marker=\"o\", label=\"R@1\")\n",
    "sns.lineplot(data=df, x=\"Bin labels\", y=\"R@8\",color=\"green\",marker=\"o\", label=\"R@8\")\n",
    "plt.xlabel(\"Avg document count\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Binned author scores\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "df[\"Total docs\"].hist(bins=7)\n",
    "plt.title(\"Document counts per author\")\n",
    "plt.xlabel(\"# of documents\")\n",
    "plt.ylabel(\"# of authors\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains deprecated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATS_ACCS = [0.0, 0.05714285714285714, 0.11428571428571428, 0.17142857142857143, 0.2, 0.2, 0.2857142857142857, 0.5428571428571428]\n",
    "HALF_FEATS_ACCS = [0.0, 0.02857142857142857, 0.08571428571428572, 0.11428571428571428, 0.22857142857142856, 0.22857142857142856, 0.22857142857142856, 0.4]\n",
    "\n",
    "old_df = pd.DataFrame(\n",
    "    {\"Full features\": ALL_FEATS_ACCS,\n",
    "     \"Half features\": HALF_FEATS_ACCS,\n",
    "     \"Bin labels\":labels}\n",
    ")\n",
    "\n",
    "\n",
    "sns.lineplot(data=old_df, x=\"Bin labels\", y=\"Full features\",color=\"blue\", label=\"Full features\")\n",
    "sns.lineplot(data=old_df, x=\"Bin labels\", y=\"Half features\", color=\"red\", label=\"Half features\")\n",
    "plt.xlabel(\"Avg document count\")\n",
    "plt.ylabel(\"R@1 score\")\n",
    "plt.title(\"R@1 Development bin scores\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discourse related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_discourse_types(path:str) -> tuple[list,list,list]:\n",
    "    \"\"\"Loads the preprocessed data and sorts it by discourse type\"\"\"\n",
    "    preprocessed = load_json(path)\n",
    "    author_ids = preprocessed.keys()\n",
    "    emails = []\n",
    "    memos = []\n",
    "    txt_msgs = []\n",
    "    essays = []\n",
    "    for author_id in author_ids:\n",
    "        for author_entry in preprocessed[author_id]:\n",
    "            dtype = author_entry[\"discourse_type\"]\n",
    "            fixed = author_entry[\"fixed_text\"].split()\n",
    "            \n",
    "            if  dtype == \"email\":\n",
    "                emails.append(fixed)\n",
    "                \n",
    "            if  dtype == \"memo\":\n",
    "                memos.append(fixed)\n",
    "                \n",
    "            if  dtype == \"text_message\":\n",
    "                txt_msgs.append(fixed)\n",
    "                \n",
    "            if  dtype == \"essay\":\n",
    "                essays.append(fixed)\n",
    "    return emails, memos, txt_msgs, essays\n",
    "  \n",
    "def get_avg_tokens(dtype:list[list[str]]) -> int:\n",
    "    \n",
    "    token_counts = []\n",
    "    for tokens in dtype:\n",
    "        token_counts.append(len(tokens))\n",
    "    return np.mean(token_counts)\n",
    "              \n",
    "            \n",
    "emails, memos, txt_msgs, essays = load_all_discourse_types(\"data/pan22/preprocessed/preprocessed_data.json\")\n",
    "\n",
    "\n",
    "print(get_avg_tokens(emails))\n",
    "print(get_avg_tokens(memos))\n",
    "print(get_avg_tokens(txt_msgs))\n",
    "print(get_avg_tokens(essays))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "dtype_df = df[[\"Emails\", \"Txt msgs\", \"Essays\", \"Memos\"]].sum()\n",
    "dtype_df.plot.bar(color=[\"teal\", \"lightpink\", \"orange\", \"brown\"])\n",
    "plt.title(\"Discourse type counts\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8426aeb6a0394c95a6dca738b4382d3e4f73a60ab3fca776cba99777e8eb1027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
