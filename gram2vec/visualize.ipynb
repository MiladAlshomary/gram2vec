{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "This notebook is meant for visualizing stuff and testing code. \n",
    "\n",
    "\n",
    "## Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import spacy\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# MUD: 1,071,477 authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting by bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nums = \"1 2 3 4 5 6 7 8\".split()\n",
    "\n",
    "dev_bin_avg_docs = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# manually recorded\n",
    "HALF_FEATS_ACCS = [0.0, 0.02857142857142857, 0.08571428571428572, 0.11428571428571428, 0.22857142857142856, 0.22857142857142856, 0.22857142857142856, 0.4]\n",
    "ALL_FEATS_ACCS = [0.0, 0.05714285714285714, 0.11428571428571428, 0.17142857142857143, 0.2, 0.2, 0.2857142857142857, 0.5428571428571428]\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "df = pd.DataFrame.from_dict({\"half_features\": HALF_FEATS_ACCS,\n",
    "                             \"all_features\" : ALL_FEATS_ACCS,\n",
    "                             \"bin\":bin_nums})\n",
    "\n",
    "\n",
    "fig = px.line(df, x=\"bin\", y=df.columns[0:2], template=\"plotly_dark\", markers=True,)\n",
    "fig.update_layout(title_text=\"Accuracy per bin\", title_x=0.5)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary stats\n",
    "\n",
    "- Mean/std \\# of tokens per document\n",
    "- Mean/std \\# of tokens per author\n",
    "- Mean/std \\# of documents per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/std tokens per document\n",
      "247.99808978032473\n",
      "220.32835614956986\n",
      "\n",
      "Mean/std tokens per author\n",
      "4636.678571428572\n",
      "1772.7924311172276\n",
      "\n",
      "Mean/std document frequency per author\n",
      "18.696428571428573\n",
      "6.114735255503541\n"
     ]
    }
   ],
   "source": [
    "data = utils.load_json(\"data/pan/preprocessed/fixed_sorted_author.json\")\n",
    "\n",
    "def get_document_token_counts(data:dict[str, list]):\n",
    "    token_counts = []\n",
    "    for author_id in data.keys():\n",
    "        for doc in data[author_id]:\n",
    "            doc_tokens = doc.split()\n",
    "            token_counts.append(len(doc_tokens))\n",
    "    return token_counts\n",
    "\n",
    "doc_token_counts = get_document_token_counts(data) \n",
    "print(\"Mean/std tokens per document\")      \n",
    "print(np.mean(doc_token_counts))\n",
    "print(np.std(doc_token_counts))\n",
    "\n",
    "\n",
    "def get_author_token_counts(data:dict[str, list[str]]) -> list[int]:\n",
    "    \n",
    "    author_to_token_counts = {}\n",
    "    for author_id in data.keys():\n",
    "        token_counts = []\n",
    "        for doc in data[author_id]:\n",
    "            doc_tokens = doc.split()\n",
    "            token_counts.append(len(doc_tokens))\n",
    "            \n",
    "        author_to_token_counts[author_id] = sum(token_counts)\n",
    "            \n",
    "    return list(author_to_token_counts.values())\n",
    "\n",
    "\n",
    "author_token_counts = get_author_token_counts(data)\n",
    "\n",
    "print(\"\\nMean/std tokens per author\")\n",
    "print(np.mean(author_token_counts))\n",
    "print(np.std(author_token_counts))\n",
    "\n",
    "\n",
    "def get_num_docs_per_author(data:dict[str, list[str]]) -> list[int]:\n",
    "    author_to_doc_freq = {}\n",
    "    for author_id in data.keys():\n",
    "        author_to_doc_freq[author_id] = len(data[author_id])\n",
    "    return list(author_to_doc_freq.values())\n",
    "\n",
    "docs_per_author = get_num_docs_per_author(data)\n",
    "\n",
    "print(\"\\nMean/std document frequency per author\")\n",
    "print(np.mean(docs_per_author))\n",
    "print(np.std(docs_per_author))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cells are for generating my potential CSE 564 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GrammarVectorizer: Old logs detected. Clearing...\n",
      "GrammarVectorizer: Done\n"
     ]
    }
   ],
   "source": [
    "from gram2vec.featurizers import GrammarVectorizer\n",
    "\n",
    "\n",
    "def vectorize_all_data(data:dict, g2v:GrammarVectorizer) -> np.ndarray:\n",
    "    \"\"\"Vectorizes a dict of documents. Returns a matrix from all documents\"\"\"\n",
    "    vectors = []\n",
    "    for author_id in data.keys():\n",
    "        for text in data[author_id]:\n",
    "            grammar_vector = g2v.vectorize(text, return_vector=False)\n",
    "            vectors.append(grammar_vector)\n",
    "    try:\n",
    "        return np.stack(vectors)\n",
    "    except:\n",
    "        return vectors\n",
    "\n",
    "def get_authors(data:dict) -> list[int]:\n",
    "    \"\"\"Get all instances of authors from data\"\"\"\n",
    "    authors = []\n",
    "    for author_id in data.keys():\n",
    "        for _ in data[author_id]:\n",
    "            authors.append(author_id)\n",
    "    return authors\n",
    "\n",
    "\n",
    "data = utils.load_json(\"data/pan/preprocessed/fixed_sorted_author.json\")\n",
    "g2v = GrammarVectorizer()\n",
    "authors = get_authors(data)\n",
    "feature_vectors = vectorize_all_data(data, g2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "from collections import defaultdict, Counter\n",
    "from more_itertools import all_unique\n",
    "\n",
    "def get_vocab(path) -> list[str]:\n",
    "    with open(path, \"r\") as fin:\n",
    "        return fin.read().strip().split(\"\\n\")\n",
    "\n",
    "pos_unigrams  = get_vocab(\"vocab/static/pos_unigrams.txt\")\n",
    "pos_bigrams   = get_vocab(\"vocab/non_static/pos_bigrams/pan/pos_bigrams.txt\")\n",
    "func_words    = get_vocab(\"vocab/static/function_words.txt\")\n",
    "punc          = get_vocab(\"vocab/static/punc_marks.txt\")\n",
    "letters       = get_vocab(\"vocab/static/letters.txt\")\n",
    "common_emojis = get_vocab(\"vocab/static/common_emojis.txt\")\n",
    "doc_stats     = [\"short_words\", \"large_words\", \"word_len_avg\", \"word_len_std\", \"sent_len_avg\", \"sent_len_std\", \"hapaxes\"]\n",
    "deps          = get_vocab(\"vocab/static/dep_labels.txt\")\n",
    "mixed_bigrams = get_vocab(\"vocab/non_static/mixed_bigrams/pan/mixed_bigrams.txt\")\n",
    "\n",
    "\n",
    "#TODO: include discourse type as a categorical variable (do tomorrow 02/20)\n",
    "all_features = pos_unigrams + pos_bigrams + func_words + punc + letters + common_emojis + doc_stats + deps + mixed_bigrams\n",
    "\n",
    "#NOTE: need to do this to ensure each feature in data viz dataset is unique\n",
    "\n",
    "\n",
    "def convert_feature_name(feature:str, seen_i, seen_a, seen_X) -> str:\n",
    "    \"\"\"Make certain conflicting feature names unique\"\"\"\n",
    "    if feature == \"i\": \n",
    "        feature = \"i (func_word)\" if not seen_i else \"i (letter)\"\n",
    "            \n",
    "    elif feature == \"a\":\n",
    "        feature = \"a (func_word)\" if not seen_a else \"a (letter)\"\n",
    "        \n",
    "    elif feature == \"X\":\n",
    "        feature = \"X (pos_unigram)\" if not seen_X else \"X (letter)\"\n",
    "        \n",
    "    return feature\n",
    "        \n",
    "\n",
    "def make_feature_to_counts_map(all_features:list[str]) -> dict[str,list]:\n",
    "    \"\"\"\n",
    "    Maps each feature to an empty list. Accounts for DIFFERENT features with the SAME label\n",
    "    \n",
    "    i.e. some distinct features have the same labels (\"i\", \"a\", \"X\"), so for data visualization purposes,\n",
    "    they need to be renamed to be distinct. This DOES NOT affect the vectors in any way. \n",
    "    \n",
    "    The conditionals here follow the same concatenation order as all_features\n",
    "    \"\"\"\n",
    "    seen_i, seen_a, seen_X = False, False, False\n",
    "    count_dict = {}\n",
    "    for feature in all_features:\n",
    "        if feature == \"i\":\n",
    "            feature = convert_feature_name(feature, seen_i, seen_a, seen_X)\n",
    "            seen_i = True\n",
    "            \n",
    "        if feature == \"a\":\n",
    "            feature = convert_feature_name(feature, seen_i, seen_a, seen_X)\n",
    "            seen_a = True\n",
    "            \n",
    "        if feature == \"X\":\n",
    "            feature = convert_feature_name(feature, seen_i, seen_a, seen_X)\n",
    "            seen_X = True\n",
    "            \n",
    "        count_dict[feature] = []\n",
    "    return count_dict\n",
    "\n",
    "            \n",
    "def populate_feature_to_counts_map(all_features:list[str], feature_vectors:list) -> dict[str,list[int]]:\n",
    "    \"\"\"\n",
    "    Populates the feature_to_count dict. Accounts for DIFFERENT features with the SAME label\n",
    "    \n",
    "    For every feature's count_dict, append the feature name's count number to \n",
    "    corresponding list in feats_to_counts\n",
    "    \"\"\"\n",
    "    feats_to_counts = make_feature_to_counts_map(all_features)\n",
    "    seen_i, seen_a, seen_X = False, False, False\n",
    "    \n",
    "    for feature in feature_vectors:\n",
    "        for count_dict in feature.count_map.values():\n",
    "            for feat_name, count in count_dict.items():\n",
    "                \n",
    "                if feat_name == \"i\":\n",
    "                    feat_name = convert_feature_name(feat_name, seen_i, seen_a, seen_X)\n",
    "                    seen_i = True\n",
    "                    \n",
    "                if feat_name == \"a\":\n",
    "                    feat_name = convert_feature_name(feat_name, seen_i, seen_a, seen_X)\n",
    "                    seen_a = True\n",
    "                    \n",
    "                if feat_name == \"X\":\n",
    "                    feat_name = convert_feature_name(feat_name, seen_i, seen_a, seen_X)\n",
    "                    seen_X = True\n",
    "                       \n",
    "                feats_to_counts[str(feat_name)].append(count)\n",
    "        seen_i, seen_a, seen_X = False, False, False # reset flags for every count_dict\n",
    "            \n",
    "    return feats_to_counts\n",
    "\n",
    "features_to_count_lists = populate_feature_to_counts_map(all_features, feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! REMEMBER TO ADD DISCOURSE TYPES\n",
    "df = pd.DataFrame(features_to_count_lists)\n",
    "df.insert(0, \"author_id\", authors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from more_itertools import collapse\n",
    "\n",
    "def get_data(path) -> list[dict]:\n",
    "    \"\"\"Reads a series of JSON objects into a list\"\"\"\n",
    "    return [json.loads(line) for line in open(path, \"r\")]\n",
    "\n",
    "def load_raw_data(pairs_path:str, truths_path:str):\n",
    "    \"\"\"This function loads the raw json data as a list of dicts and extracts each pair\"\"\"\n",
    "    pairs = get_data(pairs_path)\n",
    "    truths = get_data(truths_path)\n",
    "    #id_pairs = [tuple(entry[\"authors\"]) for entry in truths]\n",
    "    doc_pairs = [tuple(entry[\"pair\"]) for entry in pairs]\n",
    "    discourse_pairs = [tuple(entry[\"discourse_types\"]) for entry in pairs]\n",
    "\n",
    "    return doc_pairs, discourse_pairs\n",
    "\n",
    "def get_document_discourse_types(doc_pairs:list[tuple], discourse_pairs:list[tuple]) -> list[str]:\n",
    "    \"\"\"Gets all document discourse types. Accounts for duplicate documents seen document pairings\"\"\"\n",
    "    discourse_types = []\n",
    "    seen_docs = []\n",
    "    for text_pair, discourse_pair in zip(doc_pairs, discourse_pairs):\n",
    "        for document, discourse in zip(text_pair, discourse_pair):\n",
    "            if document not in seen_docs:\n",
    "                discourse_types.append(discourse)\n",
    "                seen_docs.append(document)\n",
    "    return discourse_types\n",
    "        \n",
    "\n",
    "\n",
    "doc_pairs, discourse_pairs = load_raw_data(\"data/pan/raw/pairs.jsonl\", \"data/pan/raw/truth.jsonl\")\n",
    "discourse_types = get_document_discourse_types(doc_pairs, discourse_pairs)\n",
    "\n",
    "assert sum(Counter(discourse_types).values()) == 1047\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8426aeb6a0394c95a6dca738b4382d3e4f73a60ab3fca776cba99777e8eb1027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
