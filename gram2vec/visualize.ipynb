{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "This notebook is meant for visualizing stuff and testing code. \n",
    "\n",
    "\n",
    "## Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import spacy\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# MUD: 1,071,477 authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting by bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nums = \"1 2 3 4 5 6 7 8\".split()\n",
    "\n",
    "dev_bin_avg_docs = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# manually recorded\n",
    "HALF_FEATS_ACCS = [0.0, 0.02857142857142857, 0.08571428571428572, 0.11428571428571428, 0.22857142857142856, 0.22857142857142856, 0.22857142857142856, 0.4]\n",
    "ALL_FEATS_ACCS = [0.0, 0.05714285714285714, 0.11428571428571428, 0.17142857142857143, 0.2, 0.2, 0.2857142857142857, 0.5428571428571428]\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "df = pd.DataFrame.from_dict({\"half_features\": HALF_FEATS_ACCS,\n",
    "                             \"all_features\" : ALL_FEATS_ACCS,\n",
    "                             \"bin\":bin_nums})\n",
    "\n",
    "\n",
    "fig = px.line(df, x=\"bin\", y=df.columns[0:2], template=\"plotly_dark\", markers=True,)\n",
    "fig.update_layout(title_text=\"Accuracy per bin\", title_x=0.5)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary stats\n",
    "\n",
    "- Mean/std \\# of tokens per document\n",
    "- Mean/std \\# of tokens per author\n",
    "- Mean/std \\# of documents per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean/std tokens per document\n",
      "247.99808978032473\n",
      "220.32835614956986\n",
      "\n",
      "Mean/std tokens per author\n",
      "4636.678571428572\n",
      "1772.7924311172276\n",
      "\n",
      "Mean/std document frequency per author\n",
      "18.696428571428573\n",
      "6.114735255503541\n"
     ]
    }
   ],
   "source": [
    "data = utils.load_json(\"data/pan/preprocessed/fixed_sorted_author.json\")\n",
    "\n",
    "def get_document_token_counts(data:dict[str, list]):\n",
    "    token_counts = []\n",
    "    for author_id in data.keys():\n",
    "        for doc in data[author_id]:\n",
    "            doc_tokens = doc.split()\n",
    "            token_counts.append(len(doc_tokens))\n",
    "    return token_counts\n",
    "\n",
    "doc_token_counts = get_document_token_counts(data) \n",
    "print(\"Mean/std tokens per document\")      \n",
    "print(np.mean(doc_token_counts))\n",
    "print(np.std(doc_token_counts))\n",
    "\n",
    "\n",
    "def get_author_token_counts(data:dict[str, list[str]]) -> list[int]:\n",
    "    \n",
    "    author_to_token_counts = {}\n",
    "    for author_id in data.keys():\n",
    "        token_counts = []\n",
    "        for doc in data[author_id]:\n",
    "            doc_tokens = doc.split()\n",
    "            token_counts.append(len(doc_tokens))\n",
    "            \n",
    "        author_to_token_counts[author_id] = sum(token_counts)\n",
    "            \n",
    "    return list(author_to_token_counts.values())\n",
    "\n",
    "\n",
    "author_token_counts = get_author_token_counts(data)\n",
    "\n",
    "print(\"\\nMean/std tokens per author\")\n",
    "print(np.mean(author_token_counts))\n",
    "print(np.std(author_token_counts))\n",
    "\n",
    "\n",
    "def get_num_docs_per_author(data:dict[str, list[str]]) -> list[int]:\n",
    "    author_to_doc_freq = {}\n",
    "    for author_id in data.keys():\n",
    "        author_to_doc_freq[author_id] = len(data[author_id])\n",
    "    return list(author_to_doc_freq.values())\n",
    "\n",
    "docs_per_author = get_num_docs_per_author(data)\n",
    "\n",
    "print(\"\\nMean/std document frequency per author\")\n",
    "print(np.mean(docs_per_author))\n",
    "print(np.std(docs_per_author))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cells are for generating my potential CSE 564 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GrammarVectorizer: Old logs detected. Clearing...\n",
      "GrammarVectorizer: Done\n"
     ]
    }
   ],
   "source": [
    "from gram2vec.featurizers import GrammarVectorizer\n",
    "\n",
    "\n",
    "def vectorize_all_data(data:dict, g2v:GrammarVectorizer) -> np.ndarray:\n",
    "    \"\"\"Vectorizes a dict of documents. Returns a matrix from all documents\"\"\"\n",
    "    vectors = []\n",
    "    for author_id in data.keys():\n",
    "        for text in data[author_id]:\n",
    "            grammar_vector = g2v.vectorize(text, return_vector=False)\n",
    "            vectors.append(grammar_vector)\n",
    "    try:\n",
    "        return np.stack(vectors)\n",
    "    except:\n",
    "        return vectors\n",
    "\n",
    "def get_authors(data:dict) -> list[int]:\n",
    "    \"\"\"Get all instances of authors from data\"\"\"\n",
    "    authors = []\n",
    "    for author_id in data.keys():\n",
    "        for _ in data[author_id]:\n",
    "            authors.append(author_id)\n",
    "    return authors\n",
    "\n",
    "\n",
    "data = utils.load_json(\"data/pan/preprocessed/fixed_sorted_author.json\")\n",
    "g2v = GrammarVectorizer()\n",
    "authors = get_authors(data)\n",
    "feature_vectors = vectorize_all_data(data, g2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407\n",
      "395\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "from collections import defaultdict\n",
    "\n",
    "def get_vocab(path) -> list[str]:\n",
    "    with open(path, \"r\") as fin:\n",
    "        return fin.read().strip().split(\"\\n\")\n",
    "\n",
    "pos_unigrams  = get_vocab(\"vocab/static/pos_unigrams.txt\")\n",
    "pos_bigrams   = get_vocab(\"vocab/non_static/pos_bigrams/pan/pos_bigrams.txt\")\n",
    "func_words    = get_vocab(\"vocab/static/function_words.txt\")\n",
    "punc          = get_vocab(\"vocab/static/punc_marks.txt\")\n",
    "letters       = get_vocab(\"vocab/static/letters.txt\")\n",
    "common_emojis = get_vocab(\"vocab/static/common_emojis.txt\")\n",
    "doc_stats     = [\"short_words\", \"large_words\", \"word_len_avg\", \"word_len_std\", \"sent_len_avg\", \"sent_len_std\", \"hapaxes\"]\n",
    "deps          = get_vocab(\"vocab/static/dep_labels.txt\")\n",
    "mixed_bigrams = get_vocab(\"vocab/non_static/mixed_bigrams/pan/mixed_bigrams.txt\")\n",
    "\n",
    "\n",
    "#TODO: include discourse type as a categorical variable (do tomorrow 02/20)\n",
    "all_features = pos_unigrams + pos_bigrams + func_words + punc + letters + common_emojis + doc_stats + deps + mixed_bigrams\n",
    "\n",
    "print(len(all_features))\n",
    "print(len({feat:[] for feat in all_features}))\n",
    "\n",
    "\n",
    "\n",
    "def feature_to_count_map(all_features:list[str], feature_vectors:list) -> dict[str,list]:\n",
    "    \n",
    "    feats_to_counts = defaultdict(lambda:[])\n",
    "    for feature in feature_vectors:\n",
    "        for count_dict in feature.count_map.values():\n",
    "            for feat_name, count in count_dict.items():\n",
    "                feats_to_counts[feat_name].append(count)\n",
    "               \n",
    "    return feats_to_counts\n",
    "\n",
    "\n",
    "#counts = feature_to_count_map(all_features, feature_vectors)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'b', 'c', 'c', 'd', 'd', 'e', 'e', 'f']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "from more_itertools import collapse\n",
    "tokens = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "f = list(bigrams(tokens))\n",
    "list(collapse(f))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8426aeb6a0394c95a6dca738b4382d3e4f73a60ab3fca776cba99777e8eb1027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
