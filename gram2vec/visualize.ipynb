{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "This notebook is meant for visualizing stuff and testing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from nltk import Tree\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "from more_itertools import chunked\n",
    "from dataclasses import dataclass\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from gram2vec.featurizers import GrammarVectorizer, make_document\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAN 2022 Summary Stats & Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Author:\n",
    "    \"\"\"\n",
    "    Stores author information in an easy to work with format\n",
    "    \n",
    "    :param author_id: unique author id\n",
    "    :param fixed_texts: list of author documents with regex fixes\n",
    "    :param raw_texts: list of author documents without regex fixes\n",
    "    :param discourse_types: list of discourse types\n",
    "    \n",
    "    Note: fixed_docs, raw_docs, and discourse_types are all 1 - 1 corresponding\n",
    "    \"\"\"\n",
    "    author_id:str\n",
    "    fixed_texts:list[str]\n",
    "    raw_texts:list[str]\n",
    "    discourse_types:list[str]\n",
    "    \n",
    "    def get_token_counts(self) -> list[int]:\n",
    "        return [len(word_tokenize(author_doc)) for author_doc in self.fixed_texts]\n",
    "    \n",
    "    def get_total_docs(self) -> int:\n",
    "        return len(self.fixed_texts)\n",
    "    \n",
    "    def count_dicourse_type(self, dtype:str) -> int:\n",
    "        return Counter(self.discourse_types)[dtype]\n",
    "        \n",
    "def load_json(path:str) -> dict[str, list[dict]]:\n",
    "    with open(path, \"r\") as fin:\n",
    "        data = json.load(fin)\n",
    "        return data\n",
    "\n",
    "def extract_from_dict(author_entry:dict, to_extract:str) -> list[str]:\n",
    "    return [entry[to_extract] for entry in author_entry]\n",
    "    \n",
    "def create_author_list(preprocessed_data:dict[str, list[dict]]) -> list[Author]:\n",
    "    \"\"\"\n",
    "    Converts the preprocessed data into a list of Author objects\n",
    "    \"\"\"\n",
    "    authors = []\n",
    "    for author_id in preprocessed_data.keys():\n",
    "        author_entry = preprocessed_data[author_id]\n",
    "        fixed_texts = extract_from_dict(author_entry,\"fixed_text\")\n",
    "        raw_texts = extract_from_dict(author_entry,\"raw_text\")\n",
    "        discourse_types = extract_from_dict(author_entry,\"discourse_type\")\n",
    "            \n",
    "        authors.append(Author(author_id, fixed_texts, raw_texts, discourse_types))\n",
    "        \n",
    "    return authors\n",
    "\n",
    "def get_doc_token_stats(authors:list[Author]) -> tuple[float, float]:\n",
    "    \"\"\"Gets the mean and std of tokens per document\"\"\"\n",
    "    all_doc_token_counts = []\n",
    "    for author in authors:\n",
    "        all_doc_token_counts.extend(author.get_token_counts())\n",
    "    return np.mean(all_doc_token_counts), np.std(all_doc_token_counts)\n",
    "    \n",
    "def make_author_df(authors:list[Author]) -> pd.DataFrame:\n",
    "    \n",
    "    author_maps = defaultdict(list)\n",
    "    for author in authors:\n",
    "        author_maps[\"author_id\"].append(author.author_id)\n",
    "        author_maps[\"total_token_count\"].append(sum(author.get_token_counts()))\n",
    "        author_maps[\"Total docs\"].append(author.get_total_docs())\n",
    "        author_maps[\"Emails\"].append(author.count_dicourse_type(\"email\"))\n",
    "        author_maps[\"Memos\"].append(author.count_dicourse_type(\"memo\"))\n",
    "        author_maps[\"Txt msgs\"].append(author.count_dicourse_type(\"text_message\"))\n",
    "        author_maps[\"Essays\"].append(author.count_dicourse_type(\"essay\"))\n",
    "        \n",
    "    return pd.DataFrame(author_maps)\n",
    "\n",
    "data = load_json(\"data/pan22/preprocessed/author_doc_mappings.json\")\n",
    "all_authors = create_author_list(data)\n",
    "df = make_author_df(all_authors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2v = GrammarVectorizer()\n",
    "\n",
    "def get_all_documents(data_path:str, text_type=\"fixed_text\") -> list[str]:\n",
    "    \"\"\"Aggregates all documents into one list\"\"\"\n",
    "    all_documents = []\n",
    "    for author_entries in load_json(data_path).values():\n",
    "        for entry in author_entries:\n",
    "            all_documents.append(make_document(entry[text_type], g2v.nlp))\n",
    "            \n",
    "    return all_documents\n",
    "\n",
    "all_documents = get_all_documents(\"eval/pan22_splits/knn/train.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morph tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PronType -> {'Prs': 17778, 'Art': 9760, 'Dem': 2774, 'Ind': 371, 'Rel': 356}\n",
      "\n",
      "Gender -> {'Neut': 2172, 'Fem': 285, 'Masc': 229}\n",
      "\n",
      "Case -> {'Nom': 10813, 'Acc': 3539}\n",
      "\n",
      "Definite -> {'Def': 6510, 'Ind': 3250}\n",
      "\n",
      "VerbForm -> {'Fin': 18230, 'Part': 7699, 'Inf': 9688, 'Ger': 181}\n",
      "\n",
      "Tense -> {'Pres': 14963, 'Past': 7710}\n",
      "\n",
      "Aspect -> {'Prog': 3791, 'Perf': 3535}\n",
      "\n",
      "Person -> {'1': 11166, '3': 8652, '2': 3799}\n",
      "\n",
      "Vector length: 23\n"
     ]
    }
   ],
   "source": [
    "from more_itertools import flatten\n",
    "\n",
    "def get_morph_tag_distr(documents:list, morph_tag:str) -> dict:\n",
    "    \n",
    "    morph_tag_map = defaultdict(int)\n",
    "    for document in documents:\n",
    "        doc_morphs = list(flatten([token.morph.get(morph_tag) for token in document.spacy_doc]))\n",
    "        for morph in doc_morphs:\n",
    "            morph_tag_map[morph] += 1\n",
    "    print(f\"{morph_tag} -> {dict(morph_tag_map)}\\n\")\n",
    "    return morph_tag_map\n",
    "\n",
    "def get_labels(*args:defaultdict) -> list[str]:\n",
    "    labels = []\n",
    "    for arg in args:\n",
    "        labels.extend(list(arg.keys()))\n",
    "    return labels\n",
    "\n",
    "pron_type = get_morph_tag_distr(all_documents, 'PronType')\n",
    "gender = get_morph_tag_distr(all_documents, 'Gender')\n",
    "case = get_morph_tag_distr(all_documents, 'Case')\n",
    "definite = get_morph_tag_distr(all_documents, 'Definite')\n",
    "verb_form = get_morph_tag_distr(all_documents, 'VerbForm')\n",
    "tense = get_morph_tag_distr(all_documents, 'Tense')\n",
    "aspect = get_morph_tag_distr(all_documents, 'Aspect')\n",
    "person = get_morph_tag_distr(all_documents, 'Person')\n",
    "\n",
    "labels = get_labels(pron_type, gender, case, definite, verb_form, tense, aspect, person)\n",
    "print(f\"Vector length: {len(labels)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic construction featues?\n",
    "\n",
    "- Look for spacy tree pattern matcher online\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apples nsubj 0 1 ['are']\n",
      "that dobj 0 0 ['likes', 'Apples', 'are']\n",
      "John nsubj 0 0 ['likes', 'Apples', 'are']\n",
      "likes relcl 2 0 ['Apples', 'are']\n",
      "                 are_AUX_ROOT                                   \n",
      "        ______________|_______________                           \n",
      "       |                       Apples_NOUN_nsub                 \n",
      "       |                              j                         \n",
      "       |                              |                          \n",
      "       |                       likes_VERB_relcl                 \n",
      "       |               _______________|________________          \n",
      "tasty_ADJ_acomp that_PRON_dobj                  John_PROPN_nsubj\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"39e8e6e72a654bdf98e45b389e2fed9c-0\" class=\"displacy\" width=\"1100\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apples</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">likes</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">tasty</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 750.0,2.0 750.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-3\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,354.0 L578.0,342.0 562.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-39e8e6e72a654bdf98e45b389e2fed9c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"Apples that fall from trees taste good\"\n",
    "sentence = \"Apples that John likes are tasty\"\n",
    "nlp = g2v.nlp\n",
    "doc = nlp(sentence)\n",
    "\n",
    "def _to_nltk_tree(node):\n",
    "    \n",
    "    _tok_format = lambda tok: \"_\".join([tok.orth_, tok.pos_,tok.dep_])\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(_tok_format(node), [_to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return _tok_format(node)\n",
    "\n",
    "def get_nltk_tree(doc):\n",
    "    #https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy\n",
    "    return [_to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "\n",
    "def get_root(doc) -> spacy.tokens.token.Token:\n",
    "    return [token for token in doc if token.head == token][0]\n",
    "\n",
    "def get_subject(root) -> spacy.tokens.token.Token:\n",
    "    return list(root.lefts)[0]\n",
    "\n",
    "\n",
    "root = get_root(doc)\n",
    "subject = get_subject(root)\n",
    "\n",
    "for descendant in subject.subtree:\n",
    "\n",
    "    print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
    "            descendant.n_rights,\n",
    "            [ancestor.text for ancestor in descendant.ancestors])\n",
    "\n",
    "get_nltk_tree(doc)\n",
    "displacy.render(doc, style=\"dep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"you're\", \n",
    "         \"isn't\", \n",
    "         \"aren't\", \n",
    "         \"can't\", \n",
    "         \"couldn't\",\n",
    "         \"wouldn't\", \n",
    "         \"they're\", \n",
    "         \"we're\", \n",
    "         \"didn't\", \n",
    "         \"it'll\", \n",
    "         \"i'd\", \n",
    "         \"he's\",\n",
    "         \"they've\",\n",
    "         \"they'd\",\n",
    "         \"i'm\",\n",
    "         \"she's\",\n",
    "         \"someboy's\",\n",
    "         ...]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char trigrams?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab richness vector?\n",
    "\n",
    "- Hapaxes\n",
    "- (other things I forgot actually)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.scripts.pan_create_bins import get_train_authors_sorted_by_docfreq\n",
    "\n",
    "def bin_authors(iterable) -> tuple[list[str], ...]:\n",
    "    return tuple(chunked(iterable, 7)) \n",
    "\n",
    "def make_doc_avg_labels(sorted_dict):\n",
    "       \n",
    "       labels = []\n",
    "       for bin in bin_authors(list(sorted_dict.values())):\n",
    "              labels.append(round(np.mean(bin), 2))\n",
    "       return labels\n",
    "\n",
    "train_path = \"eval/pan22_splits/knn/train.json\"\n",
    "train_authors_sorted = get_train_authors_sorted_by_docfreq(train_path)\n",
    "labels = make_doc_avg_labels(train_authors_sorted)\n",
    "\n",
    "# k = 6\n",
    "\n",
    "r_at_1 = np.array([0.02857142857,0.2285714286,0.1428571429,0.1428571429,0.2285714286,0.1142857143,0.2285714286,0.5142857143])\n",
    "\n",
    "r_at_8 = np.array([0.2571428571,0.5142857143,0.5142857143,0.5142857143,0.7142857143,0.6857142857,0.7142857143,0.7428571429,])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "       \"Bin labels\": labels,\n",
    "       \"R@1\": r_at_1,\n",
    "       \"R@8\": r_at_8\n",
    "})\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "sns.lineplot(data=df, x=\"Bin labels\", y=\"R@1\",color=\"blue\",marker=\"o\", label=\"R@1\")\n",
    "sns.lineplot(data=df, x=\"Bin labels\", y=\"R@8\",color=\"green\",marker=\"o\", label=\"R@8\")\n",
    "plt.xlabel(\"Avg document count\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Binned author scores\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "df[\"Total docs\"].hist(bins=7)\n",
    "plt.title(\"Document counts per author\")\n",
    "plt.xlabel(\"# of documents\")\n",
    "plt.ylabel(\"# of authors\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains deprecated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATS_ACCS = [0.0, 0.05714285714285714, 0.11428571428571428, 0.17142857142857143, 0.2, 0.2, 0.2857142857142857, 0.5428571428571428]\n",
    "HALF_FEATS_ACCS = [0.0, 0.02857142857142857, 0.08571428571428572, 0.11428571428571428, 0.22857142857142856, 0.22857142857142856, 0.22857142857142856, 0.4]\n",
    "\n",
    "old_df = pd.DataFrame(\n",
    "    {\"Full features\": ALL_FEATS_ACCS,\n",
    "     \"Half features\": HALF_FEATS_ACCS,\n",
    "     \"Bin labels\":labels}\n",
    ")\n",
    "\n",
    "\n",
    "sns.lineplot(data=old_df, x=\"Bin labels\", y=\"Full features\",color=\"blue\", label=\"Full features\")\n",
    "sns.lineplot(data=old_df, x=\"Bin labels\", y=\"Half features\", color=\"red\", label=\"Half features\")\n",
    "plt.xlabel(\"Avg document count\")\n",
    "plt.ylabel(\"R@1 score\")\n",
    "plt.title(\"R@1 Development bin scores\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAN 2022 Discourse related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_discourse_types(path:str) -> tuple[list,list,list]:\n",
    "    \"\"\"Loads the preprocessed data and sorts it by discourse type\"\"\"\n",
    "    preprocessed = load_json(path)\n",
    "    author_ids = preprocessed.keys()\n",
    "    emails = []\n",
    "    memos = []\n",
    "    txt_msgs = []\n",
    "    essays = []\n",
    "    for author_id in author_ids:\n",
    "        for author_entry in preprocessed[author_id]:\n",
    "            dtype = author_entry[\"discourse_type\"]\n",
    "            fixed = author_entry[\"fixed_text\"].split()\n",
    "            \n",
    "            if  dtype == \"email\":\n",
    "                emails.append(fixed)\n",
    "                \n",
    "            if  dtype == \"memo\":\n",
    "                memos.append(fixed)\n",
    "                \n",
    "            if  dtype == \"text_message\":\n",
    "                txt_msgs.append(fixed)\n",
    "                \n",
    "            if  dtype == \"essay\":\n",
    "                essays.append(fixed)\n",
    "    return emails, memos, txt_msgs, essays\n",
    "  \n",
    "def get_avg_tokens(dtype:list[list[str]]) -> int:\n",
    "    \n",
    "    token_counts = []\n",
    "    for tokens in dtype:\n",
    "        token_counts.append(len(tokens))\n",
    "    return np.mean(token_counts)\n",
    "              \n",
    "            \n",
    "emails, memos, txt_msgs, essays = load_all_discourse_types(\"data/pan22/preprocessed/preprocessed_data.json\")\n",
    "\n",
    "\n",
    "print(get_avg_tokens(emails))\n",
    "print(get_avg_tokens(memos))\n",
    "print(get_avg_tokens(txt_msgs))\n",
    "print(get_avg_tokens(essays))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "dtype_df = df[[\"Emails\", \"Txt msgs\", \"Essays\", \"Memos\"]].sum()\n",
    "dtype_df.plot.bar(color=[\"teal\", \"lightpink\", \"orange\", \"brown\"])\n",
    "plt.title(\"Discourse type counts\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_df = pd.read_csv(\"data/blogs/preprocessed/blogtext.csv\", index_col=0)\n",
    "blogs_df.drop_duplicates(subset=\"text\",inplace=True)\n",
    "blogs_df.reset_index(inplace = True,drop=True)\n",
    "blogs_df.drop(['date'], axis=1, inplace=True)\n",
    "blogs_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8426aeb6a0394c95a6dca738b4382d3e4f73a60ab3fca776cba99777e8eb1027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
